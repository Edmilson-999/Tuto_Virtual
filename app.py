# app.py (parte atualizada)
import streamlit as st
import os

# Importa√ß√µes dos m√≥dulos internos
from modules.chatbot import gerar_resposta, load_llm
from modules.rag_system import processar_todos_pdfs, qa_chain, criar_base_conhecimento, inicializar_sistema_rag
from utils.helpers import listar_pdfs

# ---------------- CONFIGURA√á√ÉO ----------------
st.set_page_config(page_title="Tutor Virtual Inteligente", page_icon="ü§ñ")

st.title("üéì Tutor Virtual Inteligente com IA e RAG")
st.write("""
Bem-vindo! Este tutor combina **IA conversacional** com **busca em documentos (RAG)** 
para responder √†s suas perguntas de forma personalizada e contextualizada.
""")

# ---------------- BASE DE CONHECIMENTO ----------------
st.sidebar.header("üìö Base de Conhecimento")

pdfs = listar_pdfs()

if not pdfs:"""
app.py
Interface principal do Tutor Virtual Inteligente
Vers√£o compat√≠vel com embeddings locais e tratamento de erros melhorado
"""

import streamlit as st
import os
import sys

# Adicionar o diret√≥rio raiz ao path para imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Importa√ß√µes dos m√≥dulos internos
from modules.chatbot import gerar_resposta, load_llm
from modules.rag_system import qa_chain, criar_base_conhecimento, processar_todos_pdfs, carregar_base_conhecimento
from utils.helpers import listar_pdfs

# ---------------- CONFIGURA√á√ÉO INICIAL ----------------
st.set_page_config(
    page_title="Tutor Virtual Inteligente", 
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ---------------- ESTILOS CSS PERSONALIZADOS ----------------
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    .user-message {
        background-color: #e6f3ff;
        border-left: 4px solid #1f77b4;
    }
    .bot-message {
        background-color: #f0f8ff;
        border-left: 4px solid #ff6b6b;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ---------------- CABE√áALHO PRINCIPAL ----------------
st.markdown('<div class="main-header">üéì Tutor Virtual Inteligente</div>', unsafe_allow_html=True)
st.markdown("""
<div class="sub-header">
    Combina <strong>IA conversacional</strong> com <strong>busca em documentos (RAG)</strong> 
    para responder √†s suas perguntas de forma personalizada e contextualizada.
</div>
""", unsafe_allow_html=True)

# ---------------- SIDEBAR: CONFIGURA√á√ïES E BASE DE CONHECIMENTO ----------------
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Se√ß√£o de Modelo de IA
    st.subheader("üß† Modelo de IA")
    modelo_escolhido = st.selectbox(
        "Escolha o modelo:",
        ["Mistral", "OpenAI GPT-4"],
        index=0,
        help="Mistral √© recomendado para melhor custo-benef√≠cio"
    )
    
    # Verificar se as API keys est√£o configuradas
    st.subheader("üîë Status das APIs")
    
    openai_key = os.getenv("OPENAI_API_KEY")
    mistral_key = os.getenv("MISTRAL_API_KEY")
    
    if openai_key:
        st.success("‚úÖ OpenAI API Key configurada")
    else:
        st.error("‚ùå OpenAI API Key n√£o encontrada")
        
    if mistral_key:
        st.success("‚úÖ Mistral API Key configurada")
    else:
        st.error("‚ùå Mistral API Key n√£o encontrada")
    
    # Se√ß√£o de Base de Conhecimento
    st.header("üìö Base de Conhecimento")
    
    # Listar PDFs dispon√≠veis
    pdfs = listar_pdfs()
    
    if not pdfs:
        st.warning("""
        ‚ö†Ô∏è Nenhum documento PDF encontrado.
        
        Para usar o sistema RAG, adicione arquivos PDF na pasta `data/docs/`.
        """)
    else:
        st.success(f"üìñ {len(pdfs)} documento(s) dispon√≠vel(is):")
        for pdf in pdfs:
            st.write(f"‚Ä¢ {pdf}")
    
    # Bot√µes de gerenciamento da base de conhecimento
    st.subheader("üõ†Ô∏è Gerenciamento")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Processar PDFs", help="Processa todos os PDFs com embeddings locais"):
            if not pdfs:
                st.error("‚ùå Nenhum PDF para processar")
            else:
                with st.spinner("Processando documentos com IA local..."):
                    try:
                        success = processar_todos_pdfs()
                        if success:
                            st.success("‚úÖ Base criada com embeddings locais!")
                            # Recarregar a p√°gina para atualizar o estado
                            st.rerun()
                        else:
                            st.error("‚ùå Falha ao processar PDFs. Verifique os logs.")
                    except Exception as e:
                        st.error(f"‚ùå Erro: {e}")
    
    with col2:
        if st.button("üßπ Limpar Base", help="Limpa toda a base de conhecimento"):
            try:
                from modules.rag_system import limpar_base_conhecimento
                if limpar_base_conhecimento():
                    st.success("‚úÖ Base limpa com sucesso!")
                    st.rerun()
                else:
                    st.error("‚ùå Erro ao limpar base")
            except Exception as e:
                st.error(f"‚ùå Erro: {e}")
    
    # Informa√ß√µes do sistema
    st.markdown("---")
    st.header("‚ÑπÔ∏è Informa√ß√µes")
    
    # Verificar status do RAG
    if qa_chain is not None:
        st.success("‚úÖ Sistema RAG Ativo")
        st.caption("O tutor pode responder com base nos seus documentos")
    else:
        st.warning("‚ö†Ô∏è Sistema RAG Inativo")
        st.caption("Adicione PDFs e processe para ativar o RAG")
    
    st.info("""
    **Modo de Uso:**
    - **Com RAG**: Respostas baseadas nos seus documentos
    - **Sem RAG**: Respostas gerais do modelo de IA
    """)

# ---------------- √ÅREA PRINCIPAL: CHAT INTERATIVO ----------------
st.header("üí¨ Chat com o Tutor")

# Inicializar hist√≥rico de chat na session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Verificar disponibilidade do RAG
rag_disponivel = qa_chain is not None and pdfs

# Seletor de modo de resposta
if rag_disponivel:
    modo = st.radio(
        "**Modo de Resposta:**",
        ["Com base nos PDFs (RAG)", "Somente Chatbot Base"],
        index=0,
        horizontal=True
    )
    st.caption("üéØ **RAG**: Respostas contextuais dos seus documentos | **Base**: Respostas gerais da IA")
else:
    modo = "Somente Chatbot Base"
    st.warning("""
    üîÑ **Sistema RAG Indispon√≠vel** 
    - Adicione PDFs na pasta `data/docs/` 
    - Clique em 'Processar PDFs' na sidebar
    - Atualmente usando modo chatbot b√°sico
    """)

# Exibir hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Mostrar fontes se dispon√≠vel no modo RAG
        if message.get("sources") and modo == "Com base nos PDFs (RAG)":
            with st.expander("üìö Fontes utilizadas"):
                for i, source in enumerate(message["sources"][:2]):
                    st.write(f"**Fonte {i+1}:** {source}")

# Input de pergunta do usu√°rio
pergunta = st.chat_input("Digite sua pergunta aqui...")

# ---------------- PROCESSAMENTO DA PERGUNTA ----------------
if pergunta:
    # Adicionar pergunta do usu√°rio ao chat
    st.session_state.messages.append({"role": "user", "content": pergunta})
    with st.chat_message("user"):
        st.markdown(pergunta)

    # Gerar resposta com spinner
    with st.chat_message("assistant"):
        with st.spinner("ü§î Tutor est√° pensando..."):
            try:
                if modo == "Com base nos PDFs (RAG)" and rag_disponivel:
                    # Modo RAG - usar base de conhecimento
                    resposta_completa = qa_chain.invoke({"query": pergunta})
                    resposta_texto = resposta_completa['result']
                    
                    # Extrair fontes se dispon√≠veis
                    fontes = []
                    if 'source_documents' in resposta_completa:
                        for doc in resposta_completa['source_documents']:
                            fonte = doc.metadata.get('source', 'Documento')
                            if fonte not in fontes:
                                fontes.append(fonte)
                    
                    # Exibir resposta
                    st.markdown(resposta_texto)
                    
                    # Exibir fontes se dispon√≠veis
                    if fontes:
                        with st.expander("üìö Ver fontes utilizadas"):
                            for i, fonte in enumerate(fontes):
                                nome_arquivo = os.path.basename(fonte)
                                st.write(f"‚Ä¢ {nome_arquivo}")
                    
                    # Salvar no hist√≥rico com fontes
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": resposta_texto,
                        "sources": fontes
                    })
                    
                else:
                    # Modo Chatbot Base - apenas LLM
                    llm = load_llm(modelo=modelo_escolhido)
                    resposta_texto = gerar_resposta(pergunta, llm)
                    st.markdown(resposta_texto)
                    
                    # Salvar no hist√≥rico
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": resposta_texto
                    })
                    
            except Exception as e:
                erro_msg = f"""
                ‚ùå **Ocorreu um erro ao processar sua pergunta:**
                
                `{str(e)}`
                
                **Sugest√µes:**
                - Verifique sua conex√£o com a internet
                - Tente usar o modo "Somente Chatbot Base"
                - Verifique se as API keys est√£o configuradas corretamente
                - Recarregue a p√°gina e tente novamente
                """
                st.error(erro_msg)
                
                # Salvar mensagem de erro no hist√≥rico
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"Erro: {str(e)}"
                })

# ---------------- SE√á√ÉO DE RECOMENDA√á√ïES (FUTURO) ----------------
st.markdown("---")
st.header("üéØ Recomenda√ß√µes de Conte√∫do")

# Placeholder para o sistema de recomenda√ß√µes
st.info("""
**üöß Sistema de Recomenda√ß√µes em Desenvolvimento**

Em breve, o tutor ir√°:
- üìä Analisar seu padr√£o de perguntas
- üìö Recomendar conte√∫dos personalizados
- üéØ Sugerir t√≥picos para estudo baseado nas suas dificuldades
- üìà Acompanhar seu progresso de aprendizado
""")

# Exemplo de recomenda√ß√µes est√°ticas (para demonstra√ß√£o)
if st.session_state.messages:
    st.subheader("üí° Sugest√µes Baseadas na Conversa")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Perguntas Realizadas", len([m for m in st.session_state.messages if m["role"] == "user"]))
    
    with col2:
        st.metric("Respostas Geradas", len([m for m in st.session_state.messages if m["role"] == "assistant"]))
    
    with col3:
        if rag_disponivel:
            st.metric("Documentos na Base", len(pdfs))
        else:
            st.metric("Modo", "Chat B√°sico")

# -sidebar: CONTROLES ADICIONAIS ----------------
with st.sidebar:
    st.markdown("---")
    st.header("üîß Utilit√°rios")
    
    # Bot√£o para limpar hist√≥rico
    if st.button("üóëÔ∏è Limpar Hist√≥rico", help="Limpa todo o hist√≥rico de conversa"):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.success("Hist√≥rico limpo!")
        st.rerun()
    
    # Bot√£o para debug (apenas desenvolvimento)
    if st.checkbox("üêõ Modo Debug", help="Mostra informa√ß√µes t√©cnicas"):
        st.subheader("Informa√ß√µes de Debug")
        st.json({
            "total_mensagens": len(st.session_state.messages),
            "rag_disponivel": rag_disponivel,
            "pdfs_encontrados": len(pdfs),
            "modelo_selecionado": modelo_escolhido,
            "modo_atual": modo
        })

# ---------------- RODAP√â ----------------
st.markdown("---")
footer_col1, footer_col2, footer_col3 = st.columns([2, 1, 1])

with footer_col1:
    st.caption("""
    **Desenvolvido como parte do TCC ‚Äî Tutor Virtual Inteligente com IA e Recomenda√ß√£o de Conte√∫dos** üí°
    
    *Tecnologias: Streamlit ‚Ä¢ LangChain ‚Ä¢ ChromaDB ‚Ä¢ Mistral AI ‚Ä¢ Sentence Transformers*
    """)

with footer_col2:
    st.caption("""
    **Status:**
    - ü§ñ Chat: ‚úÖ Ativo
    - üìö RAG: {} 
    - üéØ Recomenda√ß√µes: üöß Desenvolvimento
    """.format("‚úÖ Ativo" if rag_disponivel else "‚ùå Inativo"))

with footer_col3:
    st.caption("""
    **Ajuda:**
    - üí¨ Fa√ßa perguntas espec√≠ficas
    - üìÅ Adicione PDFs para contexto
    - üîÑ Processe os documentos
    - üéØ Use o modo RAG para respostas precisas
    """)

# ---------------- FUN√á√ïES AUXILIARES ----------------
def verificar_dependencias():
    """
    Verifica se todas as depend√™ncias est√£o dispon√≠veis
    """
    try:
        from modules.rag_system import carregar_embeddings_locais
        embeddings = carregar_embeddings_locais()
        return True, "‚úÖ Todas as depend√™ncias est√£o OK"
    except Exception as e:
        return False, f"‚ùå Erro nas depend√™ncias: {e}"

# Verifica√ß√£o autom√°tica ao carregar (apenas no primeiro carregamento)
if "deps_checked" not in st.session_state:
    st.session_state.deps_checked = True
    status, mensagem = verificar_dependencias()
    
    if not status:
        st.sidebar.error(mensagem)
    else:
        st.sidebar.success("Sistema verificado e pronto!")
    st.sidebar.warning("‚ö†Ô∏è Nenhum documento encontrado em `data/docs/`.")
    st.sidebar.info("Adicione arquivos PDF na pasta `data/docs/` para o tutor poder estudar.")
else:
    st.sidebar.success(f"{len(pdfs)} documento(s) dispon√≠vel(is).")
    for pdf in pdfs:
        st.sidebar.write(f"‚Ä¢ {pdf}")

# Bot√£o para atualizar base
# No app.py, procure esta se√ß√£o e atualize:
if st.sidebar.button("üîÑ Atualizar base de conhecimento"):
    with st.spinner("Processando documentos com IA local..."):
        try:
            success = processar_todos_pdfs()
            if success:
                st.sidebar.success("‚úÖ Base criada com embeddings locais!")
            else:
                st.sidebar.error("‚ùå Falha ao processar PDFs")
        except Exception as e:
            st.sidebar.error(f"‚ùå Erro: {e}")

st.sidebar.markdown("---")
st.sidebar.info("üí¨ Use o campo abaixo para conversar com o tutor.")

# ---------------- INTERFACE DE CHAT ----------------
st.subheader("üí¨ Fa√ßa uma pergunta")

pergunta = st.text_input("Digite sua pergunta aqui:")

# Verificar se o sistema RAG est√° dispon√≠vel
rag_disponivel = qa_chain is not None and pdfs

if rag_disponivel:
    modo = st.radio(
        "Escolha o modo de resposta:",
        ["Com base nos PDFs (RAG)", "Somente Chatbot Base"],
        index=0
    )
else:
    st.info("‚ÑπÔ∏è Modo RAG indispon√≠vel. Verifique se h√° PDFs na pasta `data/docs/` e se as API keys est√£o configuradas.")
    modo = "Somente Chatbot Base"

# Op√ß√£o para alternar entre modelos
modelo_escolhido = st.selectbox(
    "üß† Escolha o modelo de IA:",
    ["Mistral", "OpenAI GPT-4"],
    index=0
)

# ---------------- PROCESSAMENTO ----------------
if pergunta:
    with st.spinner("A pensar... ü§î"):
        try:
            if modo == "Com base nos PDFs (RAG)" and rag_disponivel:
                resposta = qa_chain.invoke({"query": pergunta})
                st.markdown(f"**ü§ñ Tutor (com RAG):** {resposta['result']}")
                
                # Mostrar fontes (opcional)
                with st.expander("üìö Ver fontes utilizadas"):
                    for i, doc in enumerate(resposta.get('source_documents', [])[:2]):
                        st.write(f"Fonte {i+1}: {doc.metadata.get('source', 'Desconhecida')}")
                        st.caption(doc.page_content[:200] + "...")
            else:
                # Carregar modelo escolhido dinamicamente
                llm = load_llm(modelo=modelo_escolhido)
                resposta = gerar_resposta(pergunta, llm)
                st.markdown(f"**ü§ñ Tutor:** {resposta}")
                
        except Exception as e:
            st.error(f"‚ùå Ocorreu um erro: {e}")
            st.info("üí° Tente usar o modo 'Somente Chatbot Base' ou verifique a conex√£o com a API.")

# ---------------- RODAP√â ----------------
st.markdown("---")
st.caption("Desenvolvido como parte do TCC ‚Äî Tutor Virtual Inteligente com IA e Recomenda√ß√£o de Conte√∫dos üí°")
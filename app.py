# app.py (parte atualizada)
import streamlit as st
import os

# ImportaÃ§Ãµes dos mÃ³dulos internos
from modules.chatbot import gerar_resposta, load_llm
from modules.rag_system import processar_todos_pdfs, qa_chain, criar_base_conhecimento, inicializar_sistema_rag
from utils.helpers import listar_pdfs

# ---------------- CONFIGURAÃ‡ÃƒO ----------------
st.set_page_config(page_title="Tutor Virtual Inteligente", page_icon="ğŸ¤–")

st.title("ğŸ“ Tutor Virtual Inteligente com IA e RAG")
st.write("""
Bem-vindo! Este tutor combina **IA conversacional** com **busca em documentos (RAG)** 
para responder Ã s suas perguntas de forma personalizada e contextualizada.
""")

# ---------------- BASE DE CONHECIMENTO ----------------
st.sidebar.header("ğŸ“š Base de Conhecimento")

pdfs = listar_pdfs()

if not pdfs:"""
app.py
Interface principal do Tutor Virtual Inteligente
VersÃ£o compatÃ­vel com embeddings locais e tratamento de erros melhorado
"""

import streamlit as st
import os
import sys

# Adicionar o diretÃ³rio raiz ao path para imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ImportaÃ§Ãµes dos mÃ³dulos internos
from modules.chatbot import gerar_resposta, load_llm
from modules.rag_system import qa_chain, criar_base_conhecimento, processar_todos_pdfs, carregar_base_conhecimento
from utils.helpers import listar_pdfs

# ---------------- CONFIGURAÃ‡ÃƒO INICIAL ----------------
st.set_page_config(
    page_title="Tutor Virtual Inteligente", 
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ---------------- ESTILOS CSS PERSONALIZADOS ----------------
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    .user-message {
        background-color: #e6f3ff;
        border-left: 4px solid #1f77b4;
    }
    .bot-message {
        background-color: #f0f8ff;
        border-left: 4px solid #ff6b6b;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ---------------- CABEÃ‡ALHO PRINCIPAL ----------------
st.markdown('<div class="main-header">ğŸ“ Tutor Virtual Inteligente</div>', unsafe_allow_html=True)
st.markdown("""
<div class="sub-header">
    Combina <strong>IA conversacional</strong> com <strong>busca em documentos (RAG)</strong> 
    para responder Ã s suas perguntas de forma personalizada e contextualizada.
</div>
""", unsafe_allow_html=True)

# ---------------- SIDEBAR: CONFIGURAÃ‡Ã•ES E BASE DE CONHECIMENTO ----------------
with st.sidebar:
    st.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    # SeÃ§Ã£o de Modelo de IA
    st.subheader("ğŸ§  Modelo de IA")
    modelo_escolhido = st.selectbox(
        "Escolha o modelo:",
        ["Mistral", "OpenAI GPT-4"],
        index=0,
        help="Mistral Ã© recomendado para melhor custo-benefÃ­cio"
    )
    
    # Verificar se as API keys estÃ£o configuradas
    st.subheader("ğŸ”‘ Status das APIs")
    
    openai_key = os.getenv("OPENAI_API_KEY")
    mistral_key = os.getenv("MISTRAL_API_KEY")
    
    if openai_key:
        st.success("âœ… OpenAI API Key configurada")
    else:
        st.error("âŒ OpenAI API Key nÃ£o encontrada")
        
    if mistral_key:
        st.success("âœ… Mistral API Key configurada")
    else:
        st.error("âŒ Mistral API Key nÃ£o encontrada")
    
    # SeÃ§Ã£o de Base de Conhecimento
    st.header("ğŸ“š Base de Conhecimento")
    
    # Listar PDFs disponÃ­veis
    pdfs = listar_pdfs()
    
    if not pdfs:
        st.warning("""
        âš ï¸ Nenhum documento PDF encontrado.
        
        Para usar o sistema RAG, adicione arquivos PDF na pasta `data/docs/`.
        """)
    else:
        st.success(f"ğŸ“– {len(pdfs)} documento(s) disponÃ­vel(is):")
        for pdf in pdfs:
            st.write(f"â€¢ {pdf}")
    
    # BotÃµes de gerenciamento da base de conhecimento
    st.subheader("ğŸ› ï¸ Gerenciamento")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ Processar PDFs", help="Processa todos os PDFs com embeddings locais"):
            if not pdfs:
                st.error("âŒ Nenhum PDF para processar")
            else:
                with st.spinner("Processando documentos com IA local..."):
                    try:
                        success = processar_todos_pdfs()
                        if success:
                            st.success("âœ… Base criada com embeddings locais!")
                            # Recarregar a pÃ¡gina para atualizar o estado
                            st.rerun()
                        else:
                            st.error("âŒ Falha ao processar PDFs. Verifique os logs.")
                    except Exception as e:
                        st.error(f"âŒ Erro: {e}")
    
    with col2:
        if st.button("ğŸ§¹ Limpar Base", help="Limpa toda a base de conhecimento"):
            try:
                from modules.rag_system import limpar_base_conhecimento
                if limpar_base_conhecimento():
                    st.success("âœ… Base limpa com sucesso!")
                    st.rerun()
                else:
                    st.error("âŒ Erro ao limpar base")
            except Exception as e:
                st.error(f"âŒ Erro: {e}")
    
    # InformaÃ§Ãµes do sistema
    st.markdown("---")
    st.header("â„¹ï¸ InformaÃ§Ãµes")
    
    # Verificar status do RAG
    if qa_chain is not None:
        st.success("âœ… Sistema RAG Ativo")
        st.caption("O tutor pode responder com base nos seus documentos")
    else:
        st.warning("âš ï¸ Sistema RAG Inativo")
        st.caption("Adicione PDFs e processe para ativar o RAG")
    
    st.info("""
    **Modo de Uso:**
    - **Com RAG**: Respostas baseadas nos seus documentos
    - **Sem RAG**: Respostas gerais do modelo de IA
    """)

# ---------------- ÃREA PRINCIPAL: CHAT INTERATIVO ----------------
st.header("ğŸ’¬ Chat com o Tutor")

# Inicializar histÃ³rico de chat na session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Verificar disponibilidade do RAG
rag_disponivel = qa_chain is not None and pdfs

# Seletor de modo de resposta
if rag_disponivel:
    modo = st.radio(
        "**Modo de Resposta:**",
        ["Com base nos PDFs (RAG)", "Somente Chatbot Base"],
        index=0,
        horizontal=True
    )
    st.caption("ğŸ¯ **RAG**: Respostas contextuais dos seus documentos | **Base**: Respostas gerais da IA")
else:
    modo = "Somente Chatbot Base"
    st.warning("""
    ğŸ”„ **Sistema RAG IndisponÃ­vel** 
    - Adicione PDFs na pasta `data/docs/` 
    - Clique em 'Processar PDFs' na sidebar
    - Atualmente usando modo chatbot bÃ¡sico
    """)

# Exibir histÃ³rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Mostrar fontes se disponÃ­vel no modo RAG
        if message.get("sources") and modo == "Com base nos PDFs (RAG)":
            with st.expander("ğŸ“š Fontes utilizadas"):
                for i, source in enumerate(message["sources"][:2]):
                    st.write(f"**Fonte {i+1}:** {source}")

# Input de pergunta do usuÃ¡rio
pergunta = st.chat_input("Digite sua pergunta aqui...")

# ---------------- PROCESSAMENTO DA PERGUNTA ----------------
if pergunta:
    # Adicionar pergunta do usuÃ¡rio ao chat
    st.session_state.messages.append({"role": "user", "content": pergunta})
    with st.chat_message("user"):
        st.markdown(pergunta)

    # Gerar resposta com spinner
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” Tutor estÃ¡ pensando..."):
            try:
                if modo == "Com base nos PDFs (RAG)" and rag_disponivel:
                    # Modo RAG - usar base de conhecimento
                    resposta_completa = qa_chain.invoke({"query": pergunta})
                    resposta_texto = resposta_completa['result']
                    
                    # Extrair fontes se disponÃ­veis
                    fontes = []
                    if 'source_documents' in resposta_completa:
                        for doc in resposta_completa['source_documents']:
                            fonte = doc.metadata.get('source', 'Documento')
                            if fonte not in fontes:
                                fontes.append(fonte)
                    
                    # Exibir resposta
                    st.markdown(resposta_texto)
                    
                    # Exibir fontes se disponÃ­veis
                    if fontes:
                        with st.expander("ğŸ“š Ver fontes utilizadas"):
                            for i, fonte in enumerate(fontes):
                                nome_arquivo = os.path.basename(fonte)
                                st.write(f"â€¢ {nome_arquivo}")
                    
                    # Salvar no histÃ³rico com fontes
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": resposta_texto,
                        "sources": fontes
                    })
                    
                else:
                    # Modo Chatbot Base - apenas LLM
                    llm = load_llm(modelo=modelo_escolhido)
                    resposta_texto = gerar_resposta(pergunta, llm)
                    st.markdown(resposta_texto)
                    
                    # Salvar no histÃ³rico
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": resposta_texto
                    })
                    
            except Exception as e:
                erro_msg = f"""
                âŒ **Ocorreu um erro ao processar sua pergunta:**
                
                `{str(e)}`
                
                **SugestÃµes:**
                - Verifique sua conexÃ£o com a internet
                - Tente usar o modo "Somente Chatbot Base"
                - Verifique se as API keys estÃ£o configuradas corretamente
                - Recarregue a pÃ¡gina e tente novamente
                """
                st.error(erro_msg)
                
                # Salvar mensagem de erro no histÃ³rico
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": f"Erro: {str(e)}"
                })

# ---------------- SEÃ‡ÃƒO DE RECOMENDAÃ‡Ã•ES (FUTURO) ----------------
st.markdown("---")
st.header("ğŸ¯ RecomendaÃ§Ãµes de ConteÃºdo")

# Placeholder para o sistema de recomendaÃ§Ãµes
st.info("""
**ğŸš§ Sistema de RecomendaÃ§Ãµes em Desenvolvimento**

Em breve, o tutor irÃ¡:
- ğŸ“Š Analisar seu padrÃ£o de perguntas
- ğŸ“š Recomendar conteÃºdos personalizados
- ğŸ¯ Sugerir tÃ³picos para estudo baseado nas suas dificuldades
- ğŸ“ˆ Acompanhar seu progresso de aprendizado
""")

# Exemplo de recomendaÃ§Ãµes estÃ¡ticas (para demonstraÃ§Ã£o)
if st.session_state.messages:
    st.subheader("ğŸ’¡ SugestÃµes Baseadas na Conversa")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Perguntas Realizadas", len([m for m in st.session_state.messages if m["role"] == "user"]))
    
    with col2:
        st.metric("Respostas Geradas", len([m for m in st.session_state.messages if m["role"] == "assistant"]))
    
    with col3:
        if rag_disponivel:
            st.metric("Documentos na Base", len(pdfs))
        else:
            st.metric("Modo", "Chat BÃ¡sico")

# -sidebar: CONTROLES ADICIONAIS ----------------
with st.sidebar:
    st.markdown("---")
    st.header("ğŸ”§ UtilitÃ¡rios")
    
    # BotÃ£o para limpar histÃ³rico
    if st.button("ğŸ—‘ï¸ Limpar HistÃ³rico", help="Limpa todo o histÃ³rico de conversa"):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.success("HistÃ³rico limpo!")
        st.rerun()
    
    # BotÃ£o para debug (apenas desenvolvimento)
    if st.checkbox("ğŸ› Modo Debug", help="Mostra informaÃ§Ãµes tÃ©cnicas"):
        st.subheader("InformaÃ§Ãµes de Debug")
        st.json({
            "total_mensagens": len(st.session_state.messages),
            "rag_disponivel": rag_disponivel,
            "pdfs_encontrados": len(pdfs),
            "modelo_selecionado": modelo_escolhido,
            "modo_atual": modo
        })

# ---------------- RODAPÃ‰ ----------------
st.markdown("---")
footer_col1, footer_col2, footer_col3 = st.columns([2, 1, 1])

with footer_col1:
    st.caption("""
    **Desenvolvido como parte do TCC â€” Tutor Virtual Inteligente com IA e RecomendaÃ§Ã£o de ConteÃºdos** ğŸ’¡
    
    *Tecnologias: Streamlit â€¢ LangChain â€¢ ChromaDB â€¢ Mistral AI â€¢ Sentence Transformers*
    """)

with footer_col2:
    st.caption("""
    **Status:**
    - ğŸ¤– Chat: âœ… Ativo
    - ğŸ“š RAG: {} 
    - ğŸ¯ RecomendaÃ§Ãµes: ğŸš§ Desenvolvimento
    """.format("âœ… Ativo" if rag_disponivel else "âŒ Inativo"))

with footer_col3:
    st.caption("""
    **Ajuda:**
    - ğŸ’¬ FaÃ§a perguntas especÃ­ficas
    - ğŸ“ Adicione PDFs para contexto
    - ğŸ”„ Processe os documentos
    - ğŸ¯ Use o modo RAG para respostas precisas
    """)

# ---------------- FUNÃ‡Ã•ES AUXILIARES ----------------
def verificar_dependencias():
    """
    Verifica se todas as dependÃªncias estÃ£o disponÃ­veis
    """
    try:
        from modules.rag_system import carregar_embeddings_locais
        embeddings = carregar_embeddings_locais()
        return True, "âœ… Todas as dependÃªncias estÃ£o OK"
    except Exception as e:
        return False, f"âŒ Erro nas dependÃªncias: {e}"

# VerificaÃ§Ã£o automÃ¡tica ao carregar (apenas no primeiro carregamento)
if "deps_checked" not in st.session_state:
    st.session_state.deps_checked = True
    status, mensagem = verificar_dependencias()
    
    if not status:
        st.sidebar.error(mensagem)
    else:
        st.sidebar.success("Sistema verificado e pronto!")
    st.sidebar.warning("âš ï¸ Nenhum documento encontrado em `data/docs/`.")
    st.sidebar.info("Adicione arquivos PDF na pasta `data/docs/` para o tutor poder estudar.")
else:
    st.sidebar.success(f"{len(pdfs)} documento(s) disponÃ­vel(is).")
    for pdf in pdfs:
        st.sidebar.write(f"â€¢ {pdf}")

# BotÃ£o para atualizar base
# No app.py, procure esta seÃ§Ã£o e atualize:
if st.sidebar.button("ğŸ”„ Atualizar base de conhecimento"):
    with st.spinner("Processando documentos com IA local..."):
        try:
            success = processar_todos_pdfs()
            if success:
                st.sidebar.success("âœ… Base criada com embeddings locais!")
            else:
                st.sidebar.error("âŒ Falha ao processar PDFs")
        except Exception as e:
            st.sidebar.error(f"âŒ Erro: {e}")

st.sidebar.markdown("---")
st.sidebar.info("ğŸ’¬ Use o campo abaixo para conversar com o tutor.")

# ---------------- INTERFACE DE CHAT ----------------
st.subheader("ğŸ’¬ FaÃ§a uma pergunta")

pergunta = st.text_input("Digite sua pergunta aqui:")

# Verificar se o sistema RAG estÃ¡ disponÃ­vel
rag_disponivel = qa_chain is not None and pdfs

if rag_disponivel:
    modo = st.radio(
        "Escolha o modo de resposta:",
        ["Com base nos PDFs (RAG)", "Somente Chatbot Base"],
        index=0
    )
else:
    st.info("â„¹ï¸ Modo RAG indisponÃ­vel. Verifique se hÃ¡ PDFs na pasta `data/docs/` e se as API keys estÃ£o configuradas.")
    modo = "Somente Chatbot Base"

# OpÃ§Ã£o para alternar entre modelos
modelo_escolhido = st.selectbox(
    "ğŸ§  Escolha o modelo de IA:",
    ["Mistral", "OpenAI GPT-4"],
    index=0
)

# ---------------- PROCESSAMENTO ----------------
if pergunta:
    with st.spinner("A pensar... ğŸ¤”"):
        try:
            if modo == "Com base nos PDFs (RAG)" and rag_disponivel:
                resposta = qa_chain.invoke({"query": pergunta})
                st.markdown(f"**ğŸ¤– Tutor (com RAG):** {resposta['result']}")
                
                # Mostrar fontes (opcional)
                with st.expander("ğŸ“š Ver fontes utilizadas"):
                    for i, doc in enumerate(resposta.get('source_documents', [])[:2]):
                        st.write(f"Fonte {i+1}: {doc.metadata.get('source', 'Desconhecida')}")
                        st.caption(doc.page_content[:200] + "...")
            else:
                # Carregar modelo escolhido dinamicamente
                llm = load_llm(modelo=modelo_escolhido)
                resposta = gerar_resposta(pergunta, llm)
                st.markdown(f"**ğŸ¤– Tutor:** {resposta}")
                
        except Exception as e:
            st.error(f"âŒ Ocorreu um erro: {e}")
            st.info("ğŸ’¡ Tente usar o modo 'Somente Chatbot Base' ou verifique a conexÃ£o com a API.")

# ---------------- RODAPÃ‰ ----------------
st.markdown("---")
st.caption("Desenvolvido como parte do TCC â€” Tutor Virtual Inteligente com IA e RecomendaÃ§Ã£o de ConteÃºdos ğŸ’¡")
"""
modules/chatbot.py
M√≥dulo respons√°vel por carregar o modelo de linguagem (LLM) e gerar respostas
do Tutor Virtual Inteligente.
Compat√≠vel com:
- OpenAI (via langchain-openai)
- Mistral (via langchain-mistralai)
"""

import os
from dotenv import load_dotenv

# Importa√ß√µes principais do LangChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage

# Carregar vari√°veis de ambiente (.env)
load_dotenv()

# =======================================================================
# üîß FUN√á√ÉO: Carregar modelo de linguagem
# =======================================================================
def load_llm(modelo: str = "Mistral"):
    """
    Carrega o modelo de linguagem de acordo com a escolha do usu√°rio.
    Aceita: "Mistral" ou "OpenAI GPT-4"
    """

    if modelo == "OpenAI GPT-4":
        from langchain_openai import ChatOpenAI
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise ValueError("‚ùå Chave API da OpenAI n√£o encontrada. Defina OPENAI_API_KEY no arquivo .env.")

        print("üîπ Carregando modelo OpenAI GPT-4...")
        return ChatOpenAI( 
            model="gpt-4o-mini",  # modelo leve, r√°pido e eficiente
            temperature=0.6,
            api_key=api_key
        )

    elif modelo == "Mistral":
        from langchain_mistralai import ChatMistralAI
        api_key = os.getenv("MISTRAL_API_KEY")

        if not api_key:
            raise ValueError("‚ùå Chave API da Mistral n√£o encontrada. Defina MISTRAL_API_KEY no arquivo .env.")

        print("üîπ Carregando modelo Mistral...")
        return ChatMistralAI(
            model="mistral-large-latest",  # modelo atual da Mistral
            temperature=0.6,
            api_key=api_key
        )

    else:
        raise ValueError(f"Modelo desconhecido: {modelo}. Use 'Mistral' ou 'OpenAI GPT-4'.")


# =======================================================================
# üß© FUN√á√ÉO: Gerar resposta do chatbot
# =======================================================================
def gerar_resposta(pergunta: str, llm=None):
    """
    Gera uma resposta textual para a pergunta do usu√°rio.
    Se nenhum LLM for passado, usa o modelo padr√£o (Mistral).
    """

    if llm is None:
        llm = load_llm("Mistral")

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=(
            "√âs um tutor virtual educacional especializado em ajudar alunos. "
            "Explica conceitos com clareza, d√° exemplos e adapta o tom conforme a dificuldade."
        )),
        HumanMessage(content=pergunta)
    ])

    # Gera√ß√£o da resposta
    resposta = llm.invoke(prompt.format_messages())

    # Retorna o texto puro da resposta
    return resposta.content.strip()
